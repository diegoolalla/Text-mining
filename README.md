# Text-mining
Tarea text mining


Este ejercicio práctico de Text Mining consiste en aplicar análisis exploratorio de datos textuales y entrenamiento de modelos supervisados con datos reales de redes sociales, usando la shared-task ProfNER (Tarea 1), centrada en detectar profesiones mencionadas en tweets durante la pandemia de COVID-19; tu objetivo es construir un clasificador binario que, dado un tweet, prediga si menciona al menos una profesión (label 1) o ninguna (label 0), para poder identificar contenido relacionado con ocupaciones y analizar qué profesiones pudieron ser más vulnerables durante la crisis. Se te proporciona un notebook de Google Colab con estructura base y celdas/funciones que no deben modificarse, y un dataset en formato Hugging Face datasets con tres particiones: train (id, texto, etiqueta), validation (misma estructura) y test (id y texto, sin etiquetas reales; aunque existe un campo de etiqueta con valor -1). Las tareas que debes completar incluyen: (1) un análisis exploratorio calculando estadísticas básicas como número de documentos, distribución de clases y longitudes de texto, además de visualizaciones (por ejemplo, wordclouds) para entender el contenido; (2) seleccionar un modelo preentrenado de Hugging Face adecuado al idioma y a la naturaleza de los tweets y justificar claramente por qué lo eliges; (3) entrenar el clasificador de forma reproducible y evaluar su rendimiento en el conjunto de validación; y (4) generar predicciones para el test, aplicando el modelo entrenado y guardando la salida en un archivo .tsv con dos columnas id y label separadas por tabulador, con el nombre obligatorio APELLIDO1_APELLIDO2_NOMBRE_ejercicio1_predicciones.tsv. La entrega requiere: (1) el notebook completo con análisis, código y justificaciones, llamado APELLIDO1_APELLIDO2_NOMBRE_ejercicio1.ipynb, y (2) el archivo de predicciones con el nombre indicado; ambos deben comprimirse en un ZIP llamado APELLIDO1_APELLIDO2_NOMBRE_TM_EJERCICIO.zip. La evaluación pondera: análisis exploratorio y preprocesamiento (20%), selección y justificación del modelo (25%), formato y validez del archivo de predicciones (5%), ejecución correcta del notebook sin intervención y sin errores (10%), rendimiento del modelo en test (30%, medido con métricas como F1-score) y claridad/calidad de las explicaciones (10%); se advierte que si el archivo de predicciones no cumple el formato, el ejercicio no se evalúa, y que la máxima nota del apartado de rendimiento la obtiene el mejor F1 de la clase, bajando proporcionalmente según el F1 conseguido. También se exige que el notebook sea ejecutable de principio a fin sin intervención manual, instalando dentro del propio notebook cualquier librería que no venga por defecto en Colab, y se recomienda trabajar directamente en Colab, respetar la estructura sin tocar las celdas fijas, documentar bien cada decisión y probar primero con subconjuntos de datos para asegurarte de que todo funciona antes de entrenar el modelo completo.
